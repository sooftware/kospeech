

<!DOCTYPE html>
<!--[if IE 8]><html class="no-js lt-ie9" lang="en" > <![endif]-->
<!--[if gt IE 8]><!--> <html class="no-js" lang="en" > <!--<![endif]-->
<head>
  <meta charset="utf-8">
  
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  
  <title>Intro &mdash; KoSpeech 0.0 documentation</title>
  

  
  
  
  

  
  <script type="text/javascript" src="../_static/js/modernizr.min.js"></script>
  
    
      <script type="text/javascript" id="documentation_options" data-url_root="../" src="../_static/documentation_options.js"></script>
        <script type="text/javascript" src="../_static/jquery.js"></script>
        <script type="text/javascript" src="../_static/underscore.js"></script>
        <script type="text/javascript" src="../_static/doctools.js"></script>
    
    <script type="text/javascript" src="../_static/js/theme.js"></script>

    

  
  <link rel="stylesheet" href="../_static/css/theme.css" type="text/css" />
  <link rel="stylesheet" href="../_static/pygments.css" type="text/css" />
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="Preparation before Training" href="preparation.html" />
    <link rel="prev" title="Welcome to KoSpeech’s documentation!" href="../index.html" /> 
</head>

<body class="wy-body-for-nav">

   
  <div class="wy-grid-for-nav">
    
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >
          

          
            <a href="../index.html" class="icon icon-home"> KoSpeech
          

          
          </a>

          
            
            
              <div class="version">
                0.0
              </div>
            
          

          
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>

          
        </div>

        <div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="main navigation">
          
            
            
              
            
            
              <p class="caption"><span class="caption-text">NOTES</span></p>
<ul class="current">
<li class="toctree-l1 current"><a class="current reference internal" href="#">Intro</a><ul>
<li class="toctree-l2"><a class="reference internal" href="#crr-character-recognition-rate">( <strong>CRR</strong> : Character Recognition Rate )</a></li>
<li class="toctree-l2"><a class="reference internal" href="#features">Features</a></li>
<li class="toctree-l2"><a class="reference internal" href="#roadmap">Roadmap</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#kospeech"><code class="docutils literal notranslate"><span class="pre">kospeech</span></code></a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="#installation">Installation</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#prerequisites">Prerequisites</a></li>
<li class="toctree-l3"><a class="reference internal" href="#install-from-source">Install from source</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="#get-started">Get Started</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#step-1-data-preprocessing">Step 1: Data Preprocessing</a></li>
<li class="toctree-l3"><a class="reference internal" href="#step-2-run-main-py">Step 2: Run <code class="docutils literal notranslate"><span class="pre">main.py</span></code></a></li>
<li class="toctree-l3"><a class="reference internal" href="#step-3-run-eval-py">Step 3: Run <code class="docutils literal notranslate"><span class="pre">eval.py</span></code></a></li>
<li class="toctree-l3"><a class="reference internal" href="#checkpoints">Checkpoints</a></li>
<li class="toctree-l3"><a class="reference internal" href="#incorporating-external-language-model-in-performance-test">Incorporating External Language Model in Performance Test</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="#troubleshoots-and-contributing">Troubleshoots and Contributing</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#code-style">Code Style</a></li>
<li class="toctree-l3"><a class="reference internal" href="#reference">Reference</a></li>
<li class="toctree-l3"><a class="reference internal" href="#citing">Citing</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="preparation.html">Preparation before Training</a></li>
<li class="toctree-l1"><a class="reference internal" href="opts.html">Options</a></li>
</ul>
<p class="caption"><span class="caption-text">ARCHITECTURE</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../Model.html">Model</a></li>
</ul>
<p class="caption"><span class="caption-text">PACKAGE REFERENCE</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../Checkpoint.html">Checkpoint</a></li>
<li class="toctree-l1"><a class="reference internal" href="../Data.html">Data</a></li>
<li class="toctree-l1"><a class="reference internal" href="../Decode.html">Decode</a></li>
<li class="toctree-l1"><a class="reference internal" href="../Evaluator.html">Evaluator</a></li>
<li class="toctree-l1"><a class="reference internal" href="../Optim.html">Optim</a></li>
<li class="toctree-l1"><a class="reference internal" href="../Trainer.html">Trainer</a></li>
<li class="toctree-l1"><a class="reference internal" href="../Etc.html">Etc</a></li>
</ul>

            
          
        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap">

      
      <nav class="wy-nav-top" aria-label="top navigation">
        
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../index.html">KoSpeech</a>
        
      </nav>


      <div class="wy-nav-content">
        
        <div class="rst-content">
        
          















<div role="navigation" aria-label="breadcrumbs navigation">

  <ul class="wy-breadcrumbs">
    
      <li><a href="../index.html">Docs</a> &raquo;</li>
        
      <li>Intro</li>
    
    
      <li class="wy-breadcrumbs-aside">
        
            
            <a href="../_sources/notes/intro.md.txt" rel="nofollow"> View page source</a>
          
        
      </li>
    
  </ul>

  
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
            
  <div class="section" id="intro">
<h1>Intro<a class="headerlink" href="#intro" title="Permalink to this headline">¶</a></h1>
<p><code class="docutils literal notranslate"><span class="pre">KoSpeech</span></code> is project for E2E automatic speech recognition implemented in <a class="reference external" href="http://pytorch.org">PyTorch</a>.<br /><code class="docutils literal notranslate"><span class="pre">kospeech</span></code> has modularized and extensible components for las models, training and evalutaion, checkpoints, parsing etc.<br />We appreciate any kind of <a class="reference external" href="https://github.com/sooftware/End-to-end-Speech-Recognition/issues">feedback or contribution</a>.</p>
<p>We used <code class="docutils literal notranslate"><span class="pre">KsponSpeech</span></code> corpus which containing <strong>1000h</strong> of Korean speech data.<br />At present our model has recorded an <strong>86.98% CRR</strong>, and we are working for a higher recognition rate.<br />Also our model has recorded <strong>91.0% CRR</strong> in <code class="docutils literal notranslate"><span class="pre">Kaldi-zeroth</span> <span class="pre">corpus</span></code></p>
<div class="section" id="crr-character-recognition-rate">
<h2>( <strong>CRR</strong> : Character Recognition Rate )<a class="headerlink" href="#crr-character-recognition-rate" title="Permalink to this headline">¶</a></h2>
</div>
<div class="section" id="features">
<h2>Features<a class="headerlink" href="#features" title="Permalink to this headline">¶</a></h2>
<ul class="simple">
<li><a class="reference external" href="https://sooftware.github.io/End-to-end-Speech-Recognition/">End-to-end (E2E) automatic speech recognition</a></li>
<li><a class="reference external" href="https://sooftware.github.io/End-to-end-Speech-Recognition/notes/opts.html">Various Options</a></li>
<li><a class="reference external" href="https://sooftware.github.io/End-to-end-Speech-Recognition/Model.html#module-e2e.model.sub_layers.extractor">(VGG / DeepSpeech2) Extractor</a></li>
<li><a class="reference external" href="https://sooftware.github.io/End-to-end-Speech-Recognition/Model.html#module-e2e.model.sub_layers.maskCNN">MaskCNN &amp; pack_padded_sequence</a></li>
<li><a class="reference external" href="https://sooftware.github.io/End-to-end-Speech-Recognition/Model.html#module-e2e.model.attention">Multi-headed (location-aware / scaled dot-product) Attention</a></li>
<li><a class="reference external" href="https://sooftware.github.io/End-to-end-Speech-Recognition/Model.html#module-e2e.model.beam_search">Top K Decoding (Beam Search)</a></li>
<li><a class="reference external" href="https://sooftware.github.io/End-to-end-Speech-Recognition/Data.html#module-e2e.data.preprocess.parser">Spectrogram Parser</a></li>
<li><a class="reference external" href="https://sooftware.github.io/End-to-end-Speech-Recognition/Data.html#module-e2e.data.preprocess.parser">Delete silence</a></li>
<li><a class="reference external" href="https://sooftware.github.io/End-to-end-Speech-Recognition/Data.html#module-e2e.data.augment.spec_augment">SpecAugment</a></li>
<li><a class="reference external" href="https://sooftware.github.io/End-to-end-Speech-Recognition/Data.html#module-e2e.data.augment.noise_augment">NoiseAugment</a></li>
<li><a class="reference external" href="https://sooftware.github.io/End-to-end-Speech-Recognition/Optim.html#module-e2e.optim.loss">Label Smoothing</a></li>
<li><a class="reference external" href="https://sooftware.github.io/End-to-end-Speech-Recognition/Solver.html#module-e2e.solver.checkpoint">Save &amp; load Checkpoint</a></li>
<li><a class="reference external" href="https://sooftware.github.io/End-to-end-Speech-Recognition/Optim.html#module-e2e.optim.lr_scheduler">Learning Rate Scheduling</a></li>
<li><a class="reference external" href="https://sooftware.github.io/End-to-end-Speech-Recognition/Data.html#module-e2e.data.data_loader">Implement data loader as multi-thread for speed</a></li>
<li>Scheduled Sampling (Teacher forcing scheduling)</li>
<li>Inference with batching</li>
<li>Multi-GPU training</li>
</ul>
<p>We have referred to many papers to develop the best model possible. And tried to make the code as efficient and easy to use as possible. If you have any minor inconvenience, please let us know anytime. We will response as soon as possible.</p>
</div>
<div class="section" id="roadmap">
<h2>Roadmap<a class="headerlink" href="#roadmap" title="Permalink to this headline">¶</a></h2>
<img src="https://user-images.githubusercontent.com/42150335/83943593-5a02f680-a838-11ea-9075-53a76d1aa9c1.png" width=350> <p>End-to-end (E2E) automatic speech recognition (ASR) is an emerging paradigm in the field of neural network-based speech recognition that offers multiple benefits. Traditional “hybrid” ASR systems, which are comprised of an acoustic model, language model, and pronunciation model, require separate training of these components, each of which can be complex.</p>
<p>For example, training of an acoustic model is a multi-stage process of model training and time alignment between the speech acoustic feature sequence and output label sequence. In contrast, E2E ASR is a single integrated approach with a much simpler training pipeline with models that operate at low audio frame rates. This reduces the training time, decoding time, and allows joint optimization with downstream processing such as natural language understanding.</p>
<p>We mainly referred to following papers.</p>
<p><a class="reference external" href="https://arxiv.org/abs/1508.01211">「Listen, Attend and Spell」</a></p>
<p><a class="reference external" href="https://arxiv.org/abs/1506.07503">「Attention Based Models for Speech Recognition」</a></p>
<p><a class="reference external" href="https://arxiv.org/abs/1712.01769">「State-of-the-art Speech Recognition with Sequence-to-Sequence Models」</a></p>
<p><a class="reference external" href="https://arxiv.org/abs/1904.08779">「SpecAugment: A Simple Data Augmentation Method for Automatic Speech Recognition」</a>.</p>
<p>If you want to study the feature of audio, we recommend this papers.</p>
<p><a class="reference external" href="https://ijirae.com/volumes/vol1/issue10/27.NVEC10086.pdf">「Voice Recognition Using MFCC Algirithm」</a>.</p>
<p>Our project based on Seq2seq with Attention Architecture.</p>
<p><img alt="image" src="https://user-images.githubusercontent.com/42150335/83260135-36b2c880-a1f4-11ea-8b38-ef88dca214bf.png" /></p>
<p><code class="docutils literal notranslate"><span class="pre">Attention</span> <span class="pre">mechanism</span></code> helps finding speech alignment. We apply multi-headed (<code class="docutils literal notranslate"><span class="pre">location-aware</span></code> / <code class="docutils literal notranslate"><span class="pre">scaled</span> <span class="pre">dot-product</span></code>) attention which you can choose. Location-aware attention proposed in <code class="docutils literal notranslate"><span class="pre">Attention</span> <span class="pre">Based</span> <span class="pre">Models</span> <span class="pre">for</span> <span class="pre">Speech</span> <span class="pre">Recognition</span></code> paper and we expanded this attention to multi-head. Multi-headed scaled dot attention proposed in <code class="docutils literal notranslate"><span class="pre">Attention</span> <span class="pre">Is</span> <span class="pre">All</span> <span class="pre">You</span> <span class="pre">Need</span></code> paper.<br />You can choose between these two options as <code class="docutils literal notranslate"><span class="pre">attn_mechanism</span></code> option. Please <a class="reference external" href="https://sooftware.github.io/End-to-end-Speech-Recognition/notes/opts.html">check</a> this page.</p>
<p>Our model architeuture is as follows.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">ListenAttendSpell</span><span class="p">(</span>
  <span class="p">(</span><span class="n">listener</span><span class="p">):</span> <span class="n">Listener</span><span class="p">(</span>
    <span class="p">(</span><span class="n">extractor</span><span class="p">):</span> <span class="n">VGGExtractor</span><span class="p">(</span>
      <span class="p">(</span><span class="n">activation</span><span class="p">):</span> <span class="n">ELU</span><span class="p">(</span><span class="n">alpha</span><span class="o">=</span><span class="mf">1.0</span><span class="p">,</span> <span class="n">inplace</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
      <span class="p">(</span><span class="n">cnn</span><span class="p">):</span> <span class="n">MaskCNN</span><span class="p">(</span>
        <span class="p">(</span><span class="n">sequential</span><span class="p">):</span> <span class="n">Sequential</span><span class="p">(</span>
          <span class="p">(</span><span class="mi">0</span><span class="p">):</span> <span class="n">Conv2d</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">64</span><span class="p">,</span> <span class="n">kernel_size</span><span class="o">=</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">3</span><span class="p">),</span> <span class="n">stride</span><span class="o">=</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">),</span> <span class="n">padding</span><span class="o">=</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">),</span> <span class="n">bias</span><span class="o">=</span><span class="bp">False</span><span class="p">)</span>
          <span class="p">(</span><span class="mi">1</span><span class="p">):</span> <span class="n">ELU</span><span class="p">(</span><span class="n">alpha</span><span class="o">=</span><span class="mf">1.0</span><span class="p">,</span> <span class="n">inplace</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
          <span class="p">(</span><span class="mi">2</span><span class="p">):</span> <span class="n">BatchNorm2d</span><span class="p">(</span><span class="mi">64</span><span class="p">,</span> <span class="n">eps</span><span class="o">=</span><span class="mf">1e-05</span><span class="p">,</span> <span class="n">momentum</span><span class="o">=</span><span class="mf">0.1</span><span class="p">,</span> <span class="n">affine</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span> <span class="n">track_running_stats</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
          <span class="p">(</span><span class="mi">3</span><span class="p">):</span> <span class="n">Conv2d</span><span class="p">(</span><span class="mi">64</span><span class="p">,</span> <span class="mi">64</span><span class="p">,</span> <span class="n">kernel_size</span><span class="o">=</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">3</span><span class="p">),</span> <span class="n">stride</span><span class="o">=</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">),</span> <span class="n">padding</span><span class="o">=</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">),</span> <span class="n">bias</span><span class="o">=</span><span class="bp">False</span><span class="p">)</span>
          <span class="p">(</span><span class="mi">4</span><span class="p">):</span> <span class="n">ELU</span><span class="p">(</span><span class="n">alpha</span><span class="o">=</span><span class="mf">1.0</span><span class="p">,</span> <span class="n">inplace</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
          <span class="p">(</span><span class="mi">5</span><span class="p">):</span> <span class="n">MaxPool2d</span><span class="p">(</span><span class="n">kernel_size</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">stride</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">padding</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">dilation</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">ceil_mode</span><span class="o">=</span><span class="bp">False</span><span class="p">)</span>
          <span class="p">(</span><span class="mi">6</span><span class="p">):</span> <span class="n">BatchNorm2d</span><span class="p">(</span><span class="mi">64</span><span class="p">,</span> <span class="n">eps</span><span class="o">=</span><span class="mf">1e-05</span><span class="p">,</span> <span class="n">momentum</span><span class="o">=</span><span class="mf">0.1</span><span class="p">,</span> <span class="n">affine</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span> <span class="n">track_running_stats</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
          <span class="p">(</span><span class="mi">7</span><span class="p">):</span> <span class="n">Conv2d</span><span class="p">(</span><span class="mi">64</span><span class="p">,</span> <span class="mi">128</span><span class="p">,</span> <span class="n">kernel_size</span><span class="o">=</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">3</span><span class="p">),</span> <span class="n">stride</span><span class="o">=</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">),</span> <span class="n">padding</span><span class="o">=</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">),</span> <span class="n">bias</span><span class="o">=</span><span class="bp">False</span><span class="p">)</span>
          <span class="p">(</span><span class="mi">8</span><span class="p">):</span> <span class="n">ELU</span><span class="p">(</span><span class="n">alpha</span><span class="o">=</span><span class="mf">1.0</span><span class="p">,</span> <span class="n">inplace</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
          <span class="p">(</span><span class="mi">9</span><span class="p">):</span> <span class="n">BatchNorm2d</span><span class="p">(</span><span class="mi">128</span><span class="p">,</span> <span class="n">eps</span><span class="o">=</span><span class="mf">1e-05</span><span class="p">,</span> <span class="n">momentum</span><span class="o">=</span><span class="mf">0.1</span><span class="p">,</span> <span class="n">affine</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span> <span class="n">track_running_stats</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
          <span class="p">(</span><span class="mi">10</span><span class="p">):</span> <span class="n">Conv2d</span><span class="p">(</span><span class="mi">128</span><span class="p">,</span> <span class="mi">128</span><span class="p">,</span> <span class="n">kernel_size</span><span class="o">=</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">3</span><span class="p">),</span> <span class="n">stride</span><span class="o">=</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">),</span> <span class="n">padding</span><span class="o">=</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">),</span> <span class="n">bias</span><span class="o">=</span><span class="bp">False</span><span class="p">)</span>
          <span class="p">(</span><span class="mi">11</span><span class="p">):</span> <span class="n">ELU</span><span class="p">(</span><span class="n">alpha</span><span class="o">=</span><span class="mf">1.0</span><span class="p">,</span> <span class="n">inplace</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
          <span class="p">(</span><span class="mi">12</span><span class="p">):</span> <span class="n">MaxPool2d</span><span class="p">(</span><span class="n">kernel_size</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">stride</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">padding</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">dilation</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">ceil_mode</span><span class="o">=</span><span class="bp">False</span><span class="p">)</span>
        <span class="p">)</span>
      <span class="p">)</span>
    <span class="p">)</span>
    <span class="p">(</span><span class="n">rnn</span><span class="p">):</span> <span class="n">LSTM</span><span class="p">(</span><span class="mi">2560</span><span class="p">,</span> <span class="mi">256</span><span class="p">,</span> <span class="n">num_layers</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span> <span class="n">batch_first</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span> <span class="n">dropout</span><span class="o">=</span><span class="mf">0.3</span><span class="p">,</span> <span class="n">bidirectional</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
  <span class="p">)</span>
  <span class="p">(</span><span class="n">speller</span><span class="p">):</span> <span class="n">Speller</span><span class="p">(</span>
    <span class="p">(</span><span class="n">rnn</span><span class="p">):</span> <span class="n">LSTM</span><span class="p">(</span><span class="mi">512</span><span class="p">,</span> <span class="mi">512</span><span class="p">,</span> <span class="n">num_layers</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">batch_first</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span> <span class="n">dropout</span><span class="o">=</span><span class="mf">0.3</span><span class="p">)</span>
    <span class="p">(</span><span class="n">embedding</span><span class="p">):</span> <span class="n">Embedding</span><span class="p">(</span><span class="mi">2038</span><span class="p">,</span> <span class="mi">512</span><span class="p">)</span>
    <span class="p">(</span><span class="n">input_dropout</span><span class="p">):</span> <span class="n">Dropout</span><span class="p">(</span><span class="n">p</span><span class="o">=</span><span class="mf">0.3</span><span class="p">,</span> <span class="n">inplace</span><span class="o">=</span><span class="bp">False</span><span class="p">)</span>
    <span class="p">(</span><span class="n">out_projection</span><span class="p">):</span> <span class="n">Linear</span><span class="p">(</span><span class="n">in_features</span><span class="o">=</span><span class="mi">512</span><span class="p">,</span> <span class="n">out_features</span><span class="o">=</span><span class="mi">2038</span><span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
    <span class="p">(</span><span class="n">attention</span><span class="p">):</span> <span class="n">MultiHeadAttention</span><span class="p">(</span>
      <span class="p">(</span><span class="n">scaled_dot</span><span class="p">):</span> <span class="n">ScaledDotProductAttention</span><span class="p">()</span>
      <span class="p">(</span><span class="n">query_projection</span><span class="p">):</span> <span class="n">Linear</span><span class="p">(</span><span class="n">in_features</span><span class="o">=</span><span class="mi">512</span><span class="p">,</span> <span class="n">out_features</span><span class="o">=</span><span class="mi">512</span><span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
      <span class="p">(</span><span class="n">value_projection</span><span class="p">):</span> <span class="n">Linear</span><span class="p">(</span><span class="n">in_features</span><span class="o">=</span><span class="mi">512</span><span class="p">,</span> <span class="n">out_features</span><span class="o">=</span><span class="mi">512</span><span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
      <span class="p">(</span><span class="n">out_projection</span><span class="p">):</span> <span class="n">Linear</span><span class="p">(</span><span class="n">in_features</span><span class="o">=</span><span class="mi">1024</span><span class="p">,</span> <span class="n">out_features</span><span class="o">=</span><span class="mi">512</span><span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
    <span class="p">)</span>
  <span class="p">)</span>
<span class="p">)</span>
</pre></div>
</div>
<div class="section" id="kospeech">
<h3><code class="docutils literal notranslate"><span class="pre">kospeech</span></code><a class="headerlink" href="#kospeech" title="Permalink to this headline">¶</a></h3>
<img src="https://user-images.githubusercontent.com/42150335/83944090-d8ad6300-a83b-11ea-8a2c-2f0d9ba0e54d.png" width=700>   <p><code class="docutils literal notranslate"><span class="pre">kospeech</span></code> module has modularized and extensible components for las models, trainer, evaluator, checkpoints etc…<br />In addition, <code class="docutils literal notranslate"><span class="pre">kospeech</span></code> enables learning in a variety of environments with a simple option setting.</p>
<ul class="simple">
<li>Options</li>
</ul>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">usage</span><span class="p">:</span> <span class="n">main</span><span class="o">.</span><span class="n">py</span> <span class="p">[</span><span class="o">-</span><span class="n">h</span><span class="p">]</span> <span class="p">[</span><span class="o">--</span><span class="n">mode</span><span class="p">]</span> <span class="p">[</span><span class="o">--</span><span class="n">sample_rate</span><span class="p">]</span>
               <span class="p">[</span><span class="o">--</span><span class="n">window_size</span><span class="p">]</span> <span class="p">[</span><span class="o">--</span><span class="n">stride</span><span class="p">]</span> <span class="p">[</span><span class="o">--</span><span class="n">n_mels</span><span class="p">]</span>
               <span class="p">[</span><span class="o">--</span><span class="n">normalize</span><span class="p">]</span> <span class="p">[</span><span class="o">--</span><span class="n">del_silence</span><span class="p">]</span> <span class="p">[</span><span class="o">--</span><span class="n">input_reverse</span><span class="p">]</span>
               <span class="p">[</span><span class="o">--</span><span class="n">feature_extract_by</span><span class="p">]</span> <span class="p">[</span><span class="o">--</span><span class="n">time_mask_para</span><span class="p">]</span> <span class="p">[</span><span class="o">--</span><span class="n">freq_mask_para</span><span class="p">]</span>
               <span class="p">[</span><span class="o">--</span><span class="n">time_mask_num</span><span class="p">]</span> <span class="p">[</span><span class="o">--</span><span class="n">freq_mask_num</span><span class="p">]</span>
               <span class="p">[</span><span class="o">--</span><span class="n">use_bidirectional</span><span class="p">]</span> <span class="p">[</span><span class="o">--</span><span class="n">hidden_dim</span><span class="p">]</span>
               <span class="p">[</span><span class="o">--</span><span class="n">dropout</span><span class="p">]</span> <span class="p">[</span><span class="o">--</span><span class="n">num_heads</span><span class="p">]</span> <span class="p">[</span><span class="o">--</span><span class="n">label_smoothing</span><span class="p">]</span>
               <span class="p">[</span><span class="o">--</span><span class="n">listener_layer_size</span><span class="p">]</span> <span class="p">[</span><span class="o">--</span><span class="n">speller_layer_size</span><span class="p">]</span> <span class="p">[</span><span class="o">--</span><span class="n">rnn_type</span><span class="p">]</span>
               <span class="p">[</span><span class="o">--</span><span class="n">extractor</span><span class="p">]</span> <span class="p">[</span><span class="o">--</span><span class="n">activation</span><span class="p">]</span>
               <span class="p">[</span><span class="o">--</span><span class="n">attn_mechanism</span><span class="p">]</span> <span class="p">[</span><span class="o">--</span><span class="n">teacher_forcing_ratio</span><span class="p">]</span>
               <span class="p">[</span><span class="o">--</span><span class="n">dataset_path</span><span class="p">]</span> <span class="p">[</span><span class="o">--</span><span class="n">data_list_path</span><span class="p">]</span>
               <span class="p">[</span><span class="o">--</span><span class="n">label_path</span><span class="p">]</span> <span class="p">[</span><span class="o">--</span><span class="n">init_uniform</span><span class="p">]</span> <span class="p">[</span><span class="o">--</span><span class="n">spec_augment</span><span class="p">]</span>
               <span class="p">[</span><span class="o">--</span><span class="n">noise_augment</span><span class="p">]</span> <span class="p">[</span><span class="o">--</span><span class="n">noiseset_size</span><span class="p">]</span>
               <span class="p">[</span><span class="o">--</span><span class="n">noise_level</span><span class="p">]</span> <span class="p">[</span><span class="o">--</span><span class="n">use_cuda</span><span class="p">]</span>
               <span class="p">[</span><span class="o">--</span><span class="n">batch_size</span><span class="p">]</span> <span class="p">[</span><span class="o">--</span><span class="n">num_workers</span><span class="p">]</span>
               <span class="p">[</span><span class="o">--</span><span class="n">num_epochs</span><span class="p">]</span> <span class="p">[</span><span class="o">--</span><span class="n">init_lr</span><span class="p">]</span>
               <span class="p">[</span><span class="o">--</span><span class="n">high_plateau_lr</span><span class="p">]</span> <span class="p">[</span><span class="o">--</span><span class="n">low_plateau_lr</span><span class="p">]</span> <span class="p">[</span><span class="o">--</span><span class="n">valid_ratio</span><span class="p">]</span>
               <span class="p">[</span><span class="o">--</span><span class="n">max_len</span><span class="p">]</span> <span class="p">[</span><span class="o">--</span><span class="n">max_grad_norm</span><span class="p">]</span>
               <span class="p">[</span><span class="o">--</span><span class="n">rampup_period</span><span class="p">]</span> <span class="p">[</span><span class="o">--</span><span class="n">decay_threshold</span><span class="p">]</span> <span class="p">[</span><span class="o">--</span><span class="n">exp_decay_period</span><span class="p">]</span>
               <span class="p">[</span><span class="o">--</span><span class="n">teacher_forcing_step</span><span class="p">]</span> <span class="p">[</span><span class="o">--</span><span class="n">min_teacher_forcing_ratio</span><span class="p">]</span>
               <span class="p">[</span><span class="o">--</span><span class="n">seed</span><span class="p">]</span> <span class="p">[</span><span class="o">--</span><span class="n">save_result_every</span><span class="p">]</span>
               <span class="p">[</span><span class="o">--</span><span class="n">checkpoint_every</span><span class="p">]</span> <span class="p">[</span><span class="o">--</span><span class="n">print_every</span><span class="p">]</span> <span class="p">[</span><span class="o">--</span><span class="n">resume</span><span class="p">]</span>
</pre></div>
</div>
<p>We are constantly updating the progress of the project on the <a class="reference external" href="https://github.com/sooftware/End-to-end-Speech-Recognition/wiki">Wiki page</a>.  Please check this page.</p>
</div>
</div>
<div class="section" id="installation">
<h2>Installation<a class="headerlink" href="#installation" title="Permalink to this headline">¶</a></h2>
<p>This project recommends Python 3.7 or higher.<br />We recommend creating a new virtual environment for this project (using virtual env or conda).</p>
<div class="section" id="prerequisites">
<h3>Prerequisites<a class="headerlink" href="#prerequisites" title="Permalink to this headline">¶</a></h3>
<ul class="simple">
<li>Numpy: <code class="docutils literal notranslate"><span class="pre">pip</span> <span class="pre">install</span> <span class="pre">numpy</span></code> (Refer <a class="reference external" href="https://github.com/numpy/numpy">here</a> for problem installing Numpy).</li>
<li>Pytorch: Refer to <a class="reference external" href="http://pytorch.org/">PyTorch website</a> to install the version w.r.t. your environment.</li>
<li>Pandas: <code class="docutils literal notranslate"><span class="pre">pip</span> <span class="pre">install</span> <span class="pre">pandas</span></code> (Refer <a class="reference external" href="https://github.com/pandas-dev/pandas">here</a> for problem installing Pandas)</li>
<li>Matplotlib: <code class="docutils literal notranslate"><span class="pre">pip</span> <span class="pre">install</span> <span class="pre">matplotlib</span></code> (Refer <a class="reference external" href="https://github.com/matplotlib/matplotlib">here</a> for problem installing Matplotlib)</li>
<li>librosa: <code class="docutils literal notranslate"><span class="pre">pip</span> <span class="pre">install</span> <span class="pre">librosa</span></code> (Refer <a class="reference external" href="https://github.com/librosa/librosa">here</a> for problem installing librosa)</li>
<li>torchaudio: <code class="docutils literal notranslate"><span class="pre">pip</span> <span class="pre">install</span> <span class="pre">torchaudio</span></code> (Refer <a class="reference external" href="https://github.com/pytorch/pytorch">here</a> for problem installing torchaudio)</li>
<li>tqdm: <code class="docutils literal notranslate"><span class="pre">pip</span> <span class="pre">install</span> <span class="pre">tqdm</span></code> (Refer <a class="reference external" href="https://github.com/tqdm/tqdm">here</a> for problem installing tqdm)</li>
</ul>
</div>
<div class="section" id="install-from-source">
<h3>Install from source<a class="headerlink" href="#install-from-source" title="Permalink to this headline">¶</a></h3>
<p>Currently we only support installation from source code using setuptools. Checkout the source code and run the<br />following commands:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">pip</span> <span class="n">install</span> <span class="o">-</span><span class="n">r</span> <span class="n">requirements</span><span class="o">.</span><span class="n">txt</span>
</pre></div>
</div>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">python</span> <span class="n">setup</span><span class="o">.</span><span class="n">py</span> <span class="n">build</span>
<span class="n">python</span> <span class="n">setup</span><span class="o">.</span><span class="n">py</span> <span class="n">install</span>
</pre></div>
</div>
</div>
</div>
<div class="section" id="get-started">
<h2>Get Started<a class="headerlink" href="#get-started" title="Permalink to this headline">¶</a></h2>
<div class="section" id="step-1-data-preprocessing">
<h3>Step 1: Data Preprocessing<a class="headerlink" href="#step-1-data-preprocessing" title="Permalink to this headline">¶</a></h3>
<p>you can preprocess <code class="docutils literal notranslate"><span class="pre">KsponSpeech</span> <span class="pre">corpus</span></code> refer <a class="reference external" href="https://github.com/sooftware/KsponSpeech.preprocess">here</a>.<br />Or refer <a class="reference external" href="https://github.com/sooftware/End-to-end-Speech-Recognition/wiki/Preparation-before-Training">this documentation</a>. This documentation contains information regarding the preprocessing of <code class="docutils literal notranslate"><span class="pre">KsponSpeech</span></code>.</p>
</div>
<div class="section" id="step-2-run-main-py">
<h3>Step 2: Run <code class="docutils literal notranslate"><span class="pre">main.py</span></code><a class="headerlink" href="#step-2-run-main-py" title="Permalink to this headline">¶</a></h3>
<ul class="simple">
<li>Default setting</li>
</ul>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span>$ ./main.sh
</pre></div>
</div>
<ul class="simple">
<li>Custom setting</li>
</ul>
<div class="highlight-shell notranslate"><div class="highlight"><pre><span></span>python ./main.py --batch_size <span class="m">32</span> --num_workers <span class="m">4</span> --num_epochs <span class="m">20</span>  --use_bidirectional <span class="se">\</span>
                 --input_reverse --spec_augment --noise_augment --use_cuda --hidden_dim <span class="m">256</span> <span class="se">\</span>
                 --dropout <span class="m">0</span>.3 --num_heads <span class="m">8</span> --label_smoothing <span class="m">0</span>.1 <span class="se">\</span>
                 --listener_layer_size <span class="m">5</span> --speller_layer_size <span class="m">3</span> --rnn_type gru <span class="se">\</span>
                 --high_plateau_lr <span class="nv">$HIGH_PLATEAU_LR</span> --teacher_forcing_ratio <span class="m">1</span>.0 --valid_ratio <span class="m">0</span>.01 <span class="se">\</span>
                 --sample_rate <span class="m">16000</span> --window_size <span class="m">20</span> --stride <span class="m">10</span> --n_mels <span class="m">80</span> --normalize --del_silence <span class="se">\</span>
                 --feature_extract_by torchaudio --time_mask_para <span class="m">70</span> --freq_mask_para <span class="m">12</span> <span class="se">\</span>
                 --time_mask_num <span class="m">2</span> --freq_mask_num <span class="m">2</span> --save_result_every <span class="m">1000</span> <span class="se">\</span>
                 --checkpoint_every <span class="m">5000</span> --print_every <span class="m">10</span> --init_lr 1e-15  --init_uniform  <span class="se">\</span>
                 --mode train --dataset_path /data3/ --data_list_path ./data/data_list/xxx.csv <span class="se">\</span>
                 --max_grad_norm <span class="m">400</span> --rampup_period <span class="m">1000</span> --max_len <span class="m">80</span> --decay_threshold <span class="m">0</span>.02 <span class="se">\</span>
                 --exp_decay_period  <span class="m">160000</span> --low_plateau_lr 1e-05 --noiseset_size <span class="m">1000</span> <span class="se">\</span>
                 --noise_level <span class="m">0</span>.7 --attn_mechanism loc --teacher_forcing_step <span class="m">0</span>.05 <span class="se">\</span>
                 --min_teacher_forcing_ratio <span class="m">0</span>.7
</pre></div>
</div>
<p>You can train the model by above command.<br />If you want to train by default setting, you can train by <code class="docutils literal notranslate"><span class="pre">Defaulting</span> <span class="pre">setting</span></code> command.<br />Or if you want to train by custom setting, you can designate hyperparameters by <code class="docutils literal notranslate"><span class="pre">Custom</span> <span class="pre">setting</span></code> command.</p>
</div>
<div class="section" id="step-3-run-eval-py">
<h3>Step 3: Run <code class="docutils literal notranslate"><span class="pre">eval.py</span></code><a class="headerlink" href="#step-3-run-eval-py" title="Permalink to this headline">¶</a></h3>
<ul class="simple">
<li>Default setting</li>
</ul>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span>$ ./eval.sh
</pre></div>
</div>
<ul class="simple">
<li>Custom setting</li>
</ul>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">python</span> <span class="o">./</span><span class="nb">eval</span><span class="o">.</span><span class="n">py</span> <span class="o">-</span><span class="n">dataset_path</span> <span class="n">dataset_path</span> <span class="o">-</span><span class="n">data_list_path</span> <span class="n">data_list_path</span> \
                 <span class="o">-</span><span class="n">mode</span> <span class="nb">eval</span> <span class="o">-</span><span class="n">use_cuda</span> <span class="o">-</span><span class="n">batch_size</span> <span class="mi">32</span> <span class="o">-</span><span class="n">num_workers</span> <span class="mi">4</span> \
                 <span class="o">-</span><span class="n">use_beam_search</span> <span class="o">-</span><span class="n">k</span> <span class="mi">5</span> <span class="o">-</span><span class="n">print_every</span> <span class="mi">100</span> \
                 <span class="o">-</span><span class="n">sample_rate</span> <span class="mi">16000</span> <span class="o">--</span><span class="n">window_size</span> <span class="mi">20</span> <span class="o">--</span><span class="n">stride</span> <span class="mi">10</span> <span class="o">--</span><span class="n">n_mels</span> <span class="mi">80</span> <span class="o">-</span><span class="n">feature_extract_by</span> <span class="n">librosa</span> \
                 <span class="o">-</span><span class="n">normalize</span> <span class="o">-</span><span class="n">del_silence</span> <span class="o">-</span><span class="n">input_reverse</span> 
</pre></div>
</div>
<p>Now you have a model which you can use to predict on new data. We do this by running <code class="docutils literal notranslate"><span class="pre">beam</span> <span class="pre">search</span></code> (or <code class="docutils literal notranslate"><span class="pre">greedy</span> <span class="pre">search</span></code>).<br />Like training, you can choose between <code class="docutils literal notranslate"><span class="pre">Default</span> <span class="pre">setting</span></code> or <code class="docutils literal notranslate"><span class="pre">Custom</span> <span class="pre">setting</span></code>.</p>
</div>
<div class="section" id="checkpoints">
<h3>Checkpoints<a class="headerlink" href="#checkpoints" title="Permalink to this headline">¶</a></h3>
<p>Checkpoints are organized by experiments and timestamps as shown in the following file structure.</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">save_dir</span>
<span class="o">+--</span> <span class="n">checkpoints</span>
<span class="o">|</span>  <span class="o">+--</span> <span class="n">YYYY_mm_dd_HH_MM_SS</span>
   <span class="o">|</span>  <span class="o">+--</span> <span class="n">trainer_states</span><span class="o">.</span><span class="n">pt</span>
   <span class="o">|</span>  <span class="o">+--</span> <span class="n">model</span><span class="o">.</span><span class="n">pt</span>
</pre></div>
</div>
<p>You can resume and load from checkpoints.</p>
</div>
<div class="section" id="incorporating-external-language-model-in-performance-test">
<h3>Incorporating External Language Model in Performance Test<a class="headerlink" href="#incorporating-external-language-model-in-performance-test" title="Permalink to this headline">¶</a></h3>
<p>We introduce incorporating external language model in performance test.<br />If you are interested in this content, please check <a class="reference external" href="https://github.com/sooftware/char-rnnlm">here</a>.</p>
</div>
</div>
<div class="section" id="troubleshoots-and-contributing">
<h2>Troubleshoots and Contributing<a class="headerlink" href="#troubleshoots-and-contributing" title="Permalink to this headline">¶</a></h2>
<p>If you have any questions, bug reports, and feature requests, please <a class="reference external" href="https://github.com/sooftware/End-to-end-Speech-Recognition/issues">open an issue</a> on Github.<br />For live discussions, please go to our <a class="reference external" href="https://gitter.im/Korean-Speech-Recognition/community">gitter</a> or Contacts sh951011&#64;gmail.com please.</p>
<p>We appreciate any kind of feedback or contribution.  Feel free to proceed with small issues like bug fixes, documentation improvement.  For major contributions and new features, please discuss with the collaborators in corresponding issues.</p>
<div class="section" id="code-style">
<h3>Code Style<a class="headerlink" href="#code-style" title="Permalink to this headline">¶</a></h3>
<p>We follow <a class="reference external" href="https://www.python.org/dev/peps/pep-0008/">PEP-8</a> for code style. Especially the style of docstrings is important to generate documentation.</p>
</div>
<div class="section" id="reference">
<h3>Reference<a class="headerlink" href="#reference" title="Permalink to this headline">¶</a></h3>
<p><a class="reference external" href="https://arxiv.org/abs/1508.01211">[1] 「Listen, Attend and Spell」  &#64;Paper</a><br /><a class="reference external" href="https://arxiv.org/abs/1506.07503">[2] 「Attention Based Models for Speech Recognition」  &#64;Paper</a><br /><a class="reference external" href="https://arxiv.org/abs/1712.01769">[3] 「State-of-the-art Speech Recognition with Sequence-to-Sequence Models」   &#64;Paper</a><br /><a class="reference external" href="https://arxiv.org/abs/1904.08779">[4] 「A Simple Data Augmentation Method for Automatic Speech Recognition」  &#64;Paper</a><br /><a class="reference external" href="https://ijirae.com/volumes/vol1/issue10/27.NVEC10086.pdf">[5] 「Voice Recognition Using MFCC Algorithm」  &#64;Paper</a><br /><a class="reference external" href="https://github.com/IBM/pytorch-seq2seq">[6] IBM/pytorch-seq2seq &#64;gitHub</a><br /><a class="reference external" href="https://github.com/SeanNaren/deepspeech.pytorch">[7] SeanNaren/deepspeech.pytorch &#64;github</a><br /><a class="reference external" href="https://github.com/Alexander-H-Liu/End-to-end-ASR-Pytorch">[8] Alexander-H-Liu/End-to-end-ASR-Pytorch &#64;github</a><br /><a class="reference external" href="https://github.com/clovaai/ClovaCall">[9] clovaai/ClovaCall &#64;github</a><br /><a class="reference external" href="http://www.aihub.or.kr/aidata/105">[10] KsponSpeech &#64;AIHub</a><br /><a class="reference external" href="https://github.com/sooftware/KsponSpeech.preprocess">[11] KsponSpeech.preprocess &#64;github</a><br /><a class="reference external" href="https://sooftware.github.io/End-to-End-Korean-Speech-Recognition/">[12] Documentation</a></p>
</div>
<div class="section" id="citing">
<h3>Citing<a class="headerlink" href="#citing" title="Permalink to this headline">¶</a></h3>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="nd">@github</span><span class="p">{</span>
  <span class="n">title</span> <span class="o">=</span> <span class="p">{</span><span class="n">KoSpeech</span><span class="p">},</span>
  <span class="n">author</span> <span class="o">=</span> <span class="p">{</span><span class="n">Soohwan</span> <span class="n">Kim</span><span class="p">,</span> <span class="n">Seyoung</span> <span class="n">Bae</span><span class="p">,</span> <span class="n">Cheolhwang</span> <span class="n">Won</span><span class="p">},</span>
  <span class="n">publisher</span> <span class="o">=</span> <span class="p">{</span><span class="n">GitHub</span><span class="p">},</span>
  <span class="n">docs</span> <span class="o">=</span> <span class="p">{</span><span class="n">https</span><span class="p">:</span><span class="o">//</span><span class="n">sooftware</span><span class="o">.</span><span class="n">github</span><span class="o">.</span><span class="n">io</span><span class="o">/</span><span class="n">KoSpeech</span><span class="o">/</span><span class="p">},</span>
  <span class="n">url</span> <span class="o">=</span> <span class="p">{</span><span class="n">https</span><span class="p">:</span><span class="o">//</span><span class="n">github</span><span class="o">.</span><span class="n">com</span><span class="o">/</span><span class="n">sooftware</span><span class="o">/</span><span class="n">KoSpeech</span><span class="p">},</span>
  <span class="n">year</span> <span class="o">=</span> <span class="p">{</span><span class="mi">2020</span><span class="p">}</span>
<span class="p">}</span>
</pre></div>
</div>
</div>
</div>
</div>


           </div>
           
          </div>
          <footer>
  
    <div class="rst-footer-buttons" role="navigation" aria-label="footer navigation">
      
        <a href="preparation.html" class="btn btn-neutral float-right" title="Preparation before Training" accesskey="n" rel="next">Next <span class="fa fa-arrow-circle-right"></span></a>
      
      
        <a href="../index.html" class="btn btn-neutral float-left" title="Welcome to KoSpeech’s documentation!" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left"></span> Previous</a>
      
    </div>
  

  <hr/>

  <div role="contentinfo">
    <p>
        &copy; Copyright 2020, Soohwan Kim

    </p>
  </div>
  Built with <a href="http://sphinx-doc.org/">Sphinx</a> using a <a href="https://github.com/rtfd/sphinx_rtd_theme">theme</a> provided by <a href="https://readthedocs.org">Read the Docs</a>. 

</footer>

        </div>
      </div>

    </section>

  </div>
  


  <script type="text/javascript">
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script>

  
  
    
   

</body>
</html>